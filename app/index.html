<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Web Microphone Recorder</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f3f4f6;
        }
    </style>
</head>
<body class="bg-gray-100 flex items-center justify-center min-h-screen p-4">

    <div class="bg-white p-8 rounded-xl shadow-lg w-full max-w-lg text-center">
        <h1 class="text-3xl font-bold mb-6 text-gray-800">Microphone Recorder</h1>

        <div id="controls" class="flex justify-center space-x-4 mb-6">
            <button id="startBtn" class="bg-blue-600 text-white font-semibold py-3 px-6 rounded-lg shadow-md hover:bg-blue-700 transition-colors duration-200">
                Start Recording
            </button>
            <button id="stopBtn" class="bg-red-600 text-white font-semibold py-3 px-6 rounded-lg shadow-md hover:bg-red-700 transition-colors duration-200" disabled>
                Stop Recording
            </button>
        </div>

        <div id="status" class="text-gray-600 mb-4 font-medium">
            Status: Ready
        </div>

        <div id="audio-playback" class="mt-8 hidden">
            <h2 class="text-xl font-bold mb-4 text-gray-800">Playback</h2>
            <audio id="audioPlayer" controls class="w-full"></audio>
        </div>

        <div id="processing-results" class="mt-8 hidden">
            <h2 class="text-xl font-bold mb-4 text-gray-800">Processing Results</h2>
            <div id="resultsContent" class="bg-gray-50 text-center p-4 rounded-lg border border-gray-200 text-gray-700">
                <!-- Plot image will be displayed here -->
                <p id="processingStatus" class="font-medium text-blue-500">Processing...</p>
                <img id="f0Plot" class="mt-4 mx-auto hidden" alt="F0 Plot">
            </div>
        </div>
    </div>

    <script>
        // Get references to all the necessary DOM elements
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const statusDiv = document.getElementById('status');
        const audioPlayer = document.getElementById('audioPlayer');
        const audioPlaybackDiv = document.getElementById('audio-playback');
        const processingResultsDiv = document.getElementById('processing-results');
        const processingStatus = document.getElementById('processingStatus');
        const f0Plot = document.getElementById('f0Plot');
        const resultsContent = document.getElementById('resultsContent');

        let mediaRecorder;
        let audioChunks = [];

        // Function to start recording
        const startRecording = async () => {
            try {
                // Request access to the user's microphone
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                mediaRecorder = new MediaRecorder(stream);
                
                // Reset UI for a new recording
                audioPlaybackDiv.classList.add('hidden');
                processingResultsDiv.classList.add('hidden');
                f0Plot.classList.add('hidden');
                
                // Event handler for when data is available
                mediaRecorder.ondataavailable = event => {
                    audioChunks.push(event.data);
                };

                // Event handler for when recording stops
                mediaRecorder.onstop = () => {
                    // Combine all audio chunks into a single Blob
                    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                    audioChunks = []; // Clear the chunks for the next recording
                    
                    // Create a URL for the audio Blob for local playback
                    const audioUrl = URL.createObjectURL(audioBlob);
                    audioPlayer.src = audioUrl;
                    audioPlaybackDiv.classList.remove('hidden');

                    // Now, send the audio blob to the server for processing
                    sendAudioForProcessing(audioBlob);
                };

                // Start the recording
                mediaRecorder.start();
                statusDiv.textContent = 'Status: Recording...';
                startBtn.disabled = true;
                stopBtn.disabled = false;
            } catch (err) {
                console.error('Error accessing microphone:', err);
                statusDiv.textContent = 'Status: Error accessing microphone.';
            }
        };

        // Function to stop recording
        const stopRecording = () => {
            if (mediaRecorder && mediaRecorder.state === 'recording') {
                mediaRecorder.stop();
                // Stop all tracks on the stream to release the microphone
                mediaRecorder.stream.getTracks().forEach(track => track.stop());
                statusDiv.textContent = 'Status: Recording stopped. Sending to server for processing...';
                startBtn.disabled = false;
                stopBtn.disabled = true;
            }
        };

        // Function to send the audio blob to the Python back-end for processing
        const sendAudioForProcessing = async (audioBlob) => {
            processingResultsDiv.classList.remove('hidden');
            processingStatus.classList.remove('hidden');
            f0Plot.classList.add('hidden');

            const formData = new FormData();
            formData.append('audio', audioBlob, 'audio.wav');

            try {
                const response = await fetch('http://127.0.0.1:5000/process-audio', {
                    method: 'POST',
                    body: formData
                });

                if (response.ok) {
                    const result = await response.json();
                    if (result.plot_data) {
                        f0Plot.src = 'data:image/png;base64,' + result.plot_data;
                        f0Plot.classList.remove('hidden');
                        processingStatus.classList.add('hidden');
                        statusDiv.textContent = 'Status: Processing complete!';
                    } else {
                        throw new Error('No plot data received.');
                    }
                } else {
                    const errorText = await response.text();
                    throw new Error('Server error: ' + errorText);
                }
            } catch (error) {
                console.error('Error sending audio for processing:', error);
                processingStatus.textContent = `Error: ${error.message}`;
                processingStatus.classList.remove('hidden');
                statusDiv.textContent = 'Status: Error during processing.';
            }
        };

        // Add event listeners to the buttons
        startBtn.addEventListener('click', startRecording);
        stopBtn.addEventListener('click', stopRecording);
    </script>
</body>
</html>
